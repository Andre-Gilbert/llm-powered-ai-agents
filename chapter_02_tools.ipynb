{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02: Tools\n",
    "\n",
    "LLMs use various tools to achieve specific goals, streamline operations, and automate tasks. These tools include:\n",
    "\n",
    "1. **Data retrieval tools:** Extract information from systems or databases using APIs, SDKs, and real-time metrics.\n",
    "2. **Communication tools:** Facilitate data exchange with external stakeholders via emails, notifications, or alerts.\n",
    "3. **Data manipulation tools:** Update or modify data within systems, often requiring approval to manage operational impacts.\n",
    "\n",
    "Additional tools also exist to handle tasks LLMs struggle with, like performing calculations or accessing current date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numexpr as ne\n",
    "import dirtyjson as json\n",
    "from datetime import datetime\n",
    "from typing import Any, Callable\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from language_models.models.llm import OpenAILanguageModel, ChatMessage, ChatMessageRole\n",
    "from language_models.proxy_client import ProxyClient\n",
    "from language_models.settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_client = ProxyClient(\n",
    "    client_id=settings.CLIENT_ID,\n",
    "    client_secret=settings.CLIENT_SECRET,\n",
    "    auth_url=settings.AUTH_URL,\n",
    "    api_base=settings.API_BASE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILanguageModel(\n",
    "    proxy_client=proxy_client,\n",
    "    model=\"gpt-4\",\n",
    "    max_tokens=250,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow LLMs to leverage tools effectively, a few steps are needed. First, we need to communicate to the LLM that it has access to specific tools by providing the tool's name, a description of when or why the tool should be used, and the input arguments required for its successful execution. For our series on LLM-powered AI Agents, we will use Pydantic to simplify this process. When an LLM provides the input arguments, we need to validate them, execute the tool if the inputs are correct, and show the tool's output to the LLM. If the inputs are incorrect, we need to inform the LLM of the mistake so it can correct and resubmit the input.\n",
    "\n",
    "Tools should be represented in a format such as: Tool Name: ..., Tool Description: ..., Tool Input: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool(BaseModel):\n",
    "    \"\"\"Class that implements a tool.\n",
    "\n",
    "    Attributes:\n",
    "        function: The function that will be invoked when calling this tool.\n",
    "        name: The name of the tool.\n",
    "        description: The description of when to use the tool or what the tool does.\n",
    "        args_schema: The Pydantic model that represents the input arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    function: Callable[[Any], Any]\n",
    "    name: str\n",
    "    description: str\n",
    "    args_schema: type[BaseModel] | None = None\n",
    "\n",
    "    @property\n",
    "    def args(self) -> dict[str, Any]:\n",
    "        \"\"\"Gets the tool model JSON schema.\"\"\"\n",
    "        if self.args_schema is None:\n",
    "            return {}\n",
    "        return self.args_schema.model_json_schema()\n",
    "\n",
    "    def parse_input(self, tool_input: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Converts tool input to pydantic model.\"\"\"\n",
    "        input_args = self.args_schema\n",
    "        if input_args is not None:\n",
    "            result = input_args.model_validate(tool_input)\n",
    "            return {key: getattr(result, key) for key, _ in result.model_dump().items() if key in tool_input}\n",
    "        return tool_input\n",
    "\n",
    "    def invoke(self, tool_input: dict[str, Any]) -> Any:\n",
    "        \"\"\"Invokes a tool given arguments provided by an LLM.\"\"\"\n",
    "        try:\n",
    "            parsed_input = self.parse_input(tool_input)\n",
    "            if not parsed_input:\n",
    "                output = self.function()\n",
    "            else:\n",
    "                output = self.function(**parsed_input)\n",
    "        except (ValidationError, TypeError) as error:\n",
    "            output = \"\\n\\n\".join(\n",
    "                [\n",
    "                    f\"Could not run tool {self.name} with input:\\n{tool_input}\",\n",
    "                    f\"The error was:\\n{error}\",\n",
    "                    \"You need to correct your response\",\n",
    "                    f\"Your <input of the tool to use> must be a JSON format with the keyword arguments of:\\n{self.args}\",\n",
    "                ]\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"- Tool Name: {self.name}, Tool Description: {self.description}, Tool Input: {self.args}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, two tools are defined to be used by an LLM: \n",
    "- **A calculator tool**: The calculator tool evaluates mathematical expressions provided as strings, with an accompanying Pydantic model to validate the input.\n",
    "- **A current date tool**: The current date tool provides the current local date and time. \n",
    "\n",
    "For each tool, a name, description, function, and, if needed, argument schema are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tool Name: Calculator, Tool Description: Use this tool when you want to do calculations, Tool Input: {'properties': {'expression': {'description': 'A math expression', 'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'Calculator', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "class Calculator(BaseModel):\n",
    "    expression: str = Field(description=\"A math expression\")\n",
    "\n",
    "calculator = Tool(\n",
    "    function=lambda expression: ne.evaluate(expression).item(),\n",
    "    name=\"Calculator\",\n",
    "    description=\"Use this tool when you want to do calculations\",\n",
    "    args_schema=Calculator,\n",
    ")\n",
    "\n",
    "print(calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tool Name: Current Date, Tool Description: Use this tool to access the current local date and time, Tool Input: {}\n"
     ]
    }
   ],
   "source": [
    "current_date = Tool(\n",
    "    function=lambda: datetime.now(),\n",
    "    name=\"Current Date\",\n",
    "    description=\"Use this tool to access the current local date and time\",\n",
    ")\n",
    "\n",
    "print(current_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setup, the `system_prompt` outlines how the LLM should structure its responses when addressing a user query. This includes specifying how and when to use the defined tools to solve the problems presented by users. When the LLM encounters a user prompt that requires a calculation or a specific operation, it must adhere to a predefined structure in its response:\n",
    "\n",
    "1. **Thought**: The LLM first generates a thought, which is an explanation or reasoning about what steps need to be taken to solve the problem.\n",
    "2. **Tool**: Based on the thought, the LLM selects the appropriate tool that is designed to perform the necessary operation.\n",
    "3. **Tool Input**: After selecting the tool, the LLM decides on the correct input needed for the tool to successfully complete the task or subtask.\n",
    "\n",
    "The order of thought, tool, and tool input is crucial. This is because LLMs work by predicting the next token with the highest probability based on the previous tokens. If the context is well-defined (i.e., the thought is clearly articulated), the LLM can more accurately choose the appropriate tool. Subsequently, given the tool and the thought, the LLM can then determine the precise input required for the tool to function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The user wants to calculate the sum of several numbers. I can use the Calculator tool for this task.\n",
      "\n",
      "Tool: Calculator\n",
      "\n",
      "Tool Input: {\"expression\": \"549.72 + 6.98 + 41.00 + 35.00 + 552.00 + 76.16 + 29.12\"}\n"
     ]
    }
   ],
   "source": [
    "tools = [calculator, current_date]\n",
    "tools_str = \"\\n\\n\".join([str(tool) for tool in tools])\n",
    "\n",
    "system_prompt = f\"\"\"You are an AI assistant designed to help users with a variety of tasks.\n",
    "\n",
    "### Tools ###\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools_str}\n",
    "\n",
    "### Instructions ###\n",
    "\n",
    "Your goal is to solve the problem you will be provided with\n",
    "\n",
    "You should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Tool: <name of the tool to use>\n",
    "\n",
    "Tool Input: <input of the tool to use>\n",
    "```\n",
    "\n",
    "Your <input of the tool to use> must be a JSON format with the keyword arguments of <name of the tool to use>\"\"\"\n",
    "\n",
    "tools_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "prompt = \"Calculate the total raw cost = $549.72 + $6.98 + $41.00 + $35.00 + $552.00 + $76.16 + $29.12.\"\n",
    "\n",
    "output = llm.get_completion([\n",
    "    ChatMessage(role=ChatMessageRole.SYSTEM, content=system_prompt),\n",
    "    ChatMessage(role=ChatMessageRole.USER, content=prompt)\n",
    "])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `extract_tool_use(output: str)` is used to parse a structured response from an LLM to extract three key components: the thought process, the name of the tool to be used, and the tool's input. It uses a regular expression pattern to match these components in the output string. If the pattern does not match, it raises a ValueError with guidance on the correct response format. When a match is found, it retrieves and trims the thought, tool, and tool input from the matched groups. This function ensures that the LLM's response adheres to the expected format and extracts necessary details for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_use(output: str) -> tuple[str, str, str]:\n",
    "    pattern = r\"\\s*Thought: (.*?)\\n+Tool: ([a-zA-Z0-9_ ]+).*?\\n+Tool Input: .*?(\\{.*\\})\"\n",
    "\n",
    "    match = re.search(pattern, output, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\n",
    "            f\"You made a mistake in your response: {output}\\n\\n\"\n",
    "            + f\"You need to correct your response\\n\\n\"\n",
    "            + \"You should respond with:\\n```\\nThought: <thought process on how to respond to the prompt>\\n\\nTool: <name of the tool to use>\\n\\nTool Input: <input of the tool to use>\\n```\"\n",
    "        )\n",
    "\n",
    "    thought = match.group(1).strip()\n",
    "    tool = match.group(2).strip()\n",
    "    tool_input = match.group(3).strip()\n",
    "    return thought, tool, tool_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The user wants to calculate the sum of several numbers. I can use the Calculator tool for this task.\n",
      "Tool: Calculator\n",
      "Tool Input: {\"expression\": \"549.72 + 6.98 + 41.00 + 35.00 + 552.00 + 76.16 + 29.12\"}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    thought, tool, tool_input = extract_tool_use(output)\n",
    "    print(f\"Thought: {thought}\")\n",
    "    print(f\"Tool: {tool}\")\n",
    "    print(f\"Tool Input: {tool_input}\")\n",
    "except ValueError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the JSON representation of the tool input must be converted into a dictionary to enable the tool's invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289.98\n"
     ]
    }
   ],
   "source": [
    "tool_input = dict(json.loads(tool_input))\n",
    "tool = tools_map.get(tool)\n",
    "tool_output = tool.invoke(tool_input)\n",
    "print(tool_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the `system_prompt` defines how an LLM should handle user queries, detailing step-by-step usage of available tools and structured response formats. It includes guidance on composing responses when using tools as well as when delivering the final answer. The `prompt` presented to the LLM is a request to calculate the total raw cost of a series of amounts, augmented by the LLM's previous work, including usage of the \"Calculator\" tool. Finally, the LLM generates the final answer, following the provided instructions and prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The tool has successfully calculated the sum of the given numbers. I can now provide the user with the result.\n",
      "\n",
      "Final Answer: The total raw cost is $1289.98.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\"You are an AI assistant designed to help users with a variety of tasks.\n",
    "\n",
    "### Tools ###\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools_str}\n",
    "\n",
    "### Instructions ###\n",
    "\n",
    "Your goal is to solve the problem you will be provided with\n",
    "\n",
    "You should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Tool: <name of the tool to use>\n",
    "\n",
    "Tool Input: <input of the tool to use>\n",
    "```\n",
    "\n",
    "Your <input of the tool to use> must be a JSON format with the keyword arguments of <name of the tool to use>\n",
    "\n",
    "When you know the final answer to the user's query you should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Final Answer: <response to the prompt>\n",
    "```\n",
    "\n",
    "Your <response to the prompt> should be the final answer to the user's query\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Calculate the total raw cost = $549.72 + $6.98 + $41.00 + $35.00 + $552.00 + $76.16 + $29.12.\n",
    "\n",
    "This was your previous work:\n",
    "\n",
    "{output}\n",
    "\n",
    "Observation: Tool Output: {tool_output}\"\"\"\n",
    "\n",
    "output = llm.get_completion([\n",
    "    ChatMessage(role=ChatMessageRole.SYSTEM, content=system_prompt),\n",
    "    ChatMessage(role=ChatMessageRole.USER, content=prompt),\n",
    "])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `extract_final_answer(output: str)` is used to parse a structured response from an LLM to extract two key components: the thought process and the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(output: str) -> tuple[str, str]:\n",
    "    pattern = r\"\\s*Thought: (.*?)\\n+Final Answer:([\\s\\S]*.*?)(?:$)\"\n",
    "\n",
    "    match = re.search(pattern, output, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\n",
    "            f\"You made a mistake in your response: {output}\\n\\n\"\n",
    "            + f\"Your need to correct your response\\n\\n\"\n",
    "            + \"You should respond with:\\n```\\nThought: <thought process on how to respond to the prompt>\\n\\nFinal Answer: <response to the prompt>\\n```\"\n",
    "        )\n",
    "\n",
    "    thought = match.group(1).strip()\n",
    "    final_answer = match.group(2).strip()\n",
    "    return thought, final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The tool has successfully calculated the sum of the given numbers. I can now provide the user with the result.\n",
      "Final Answer: The total raw cost is $1289.98.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    thought, final_answer = extract_final_answer(output)\n",
    "    print(f\"Thought: {thought}\")\n",
    "    print(f\"Final Answer: {final_answer}\")\n",
    "except ValueError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same thing to retrieve the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The user is asking for the current date. I can use the \"Current Date\" tool to provide this information.\n",
      "\n",
      "Tool: Current Date\n",
      "\n",
      "Tool Input: {}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\"You are an AI assistant designed to help users with a variety of tasks.\n",
    "\n",
    "### Tools ###\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools_str}\n",
    "\n",
    "### Instructions ###\n",
    "\n",
    "Your goal is to solve the problem you will be provided with\n",
    "\n",
    "You should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Tool: <name of the tool to use>\n",
    "\n",
    "Tool Input: <input of the tool to use>\n",
    "```\n",
    "\n",
    "Your <input of the tool to use> must be a JSON format with the keyword arguments of <name of the tool to use>\n",
    "\n",
    "When you know the final answer to the user's query you should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Final Answer: <response to the prompt>\n",
    "```\n",
    "\n",
    "Your <response to the prompt> should be the final answer to the user's query\"\"\"\n",
    "\n",
    "prompt = \"What day do we have?\"\n",
    "\n",
    "output = llm.get_completion([\n",
    "    ChatMessage(role=ChatMessageRole.SYSTEM, content=system_prompt),\n",
    "    ChatMessage(role=ChatMessageRole.USER, content=prompt),\n",
    "])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The user is asking for the current date. I can use the \"Current Date\" tool to provide this information.\n",
      "Tool: Current Date\n",
      "Tool Input: {}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    thought, tool, tool_input = extract_tool_use(output)\n",
    "    print(f\"Thought: {thought}\")\n",
    "    print(f\"Tool: {tool}\")\n",
    "    print(f\"Tool Input: {tool_input}\")\n",
    "except ValueError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-16 13:56:44.245589\n"
     ]
    }
   ],
   "source": [
    "tool_input = dict(json.loads(tool_input))\n",
    "tool = tools_map.get(tool)\n",
    "tool_output = tool.invoke(tool_input)\n",
    "print(tool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The tool has provided the current date and time. The user only asked for the day, so I will extract that information from the date.\n",
      "\n",
      "Final Answer: Today is August 16, 2024.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\"You are an AI assistant designed to help users with a variety of tasks.\n",
    "\n",
    "### Tools ###\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools_str}\n",
    "\n",
    "### Instructions ###\n",
    "\n",
    "Your goal is to solve the problem you will be provided with\n",
    "\n",
    "You should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Tool: <name of the tool to use>\n",
    "\n",
    "Tool Input: <input of the tool to use>\n",
    "```\n",
    "\n",
    "Your <input of the tool to use> must be a JSON format with the keyword arguments of <name of the tool to use>\n",
    "\n",
    "When you know the final answer to the user's query you should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Final Answer: <response to the prompt>\n",
    "```\n",
    "\n",
    "Your <response to the prompt> should be the final answer to the user's query\"\"\"\n",
    "\n",
    "prompt = f\"\"\"What day do we have?\n",
    "\n",
    "This was your previous work:\n",
    "\n",
    "{output}\n",
    "\n",
    "Observation: Tool Output: {tool_output}\"\"\"\n",
    "\n",
    "output = llm.get_completion([\n",
    "    ChatMessage(role=ChatMessageRole.SYSTEM, content=system_prompt),\n",
    "    ChatMessage(role=ChatMessageRole.USER, content=prompt),\n",
    "])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The tool has provided the current date and time. The user only asked for the day, so I will extract that information from the date.\n",
      "Final Answer: Today is August 16, 2024.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    thought, final_answer = extract_final_answer(output)\n",
    "    print(f\"Thought: {thought}\")\n",
    "    print(f\"Final Answer: {final_answer}\")\n",
    "except ValueError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, it is clear that when an LLM doesn't have access to tools, we can use single completion prompts to obtain desired results. Conversely, if the LLM has tools, employing Chain-of-Thought prompting, particularly the ReAct prompting method, and requesting structured outputs that can be parsed using regular expressions becomes necessary. While outputs can be formatted in YAML or JSON, expressing outputs as plain text is more cost-effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-powered-ai-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
