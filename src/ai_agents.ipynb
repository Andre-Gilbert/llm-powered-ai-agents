{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-powered AI Agents\n",
    "\n",
    "We've explored the incredible capabilities of LLMs, the art of prompting, the integration of tools, and the generation of structured outputs. With this strong foundation, we're now ready to dive into something even more powerful - creating our very own LLM-powered AI agent.  \n",
    "\n",
    "Here are just a few examples of what they can be used for: \n",
    "- **Customer support**: Provide instant, accurate, and empathetic responses to customer inquiries, improving satisfaction and efficiency. \n",
    "- **Content creation**: Generate high-quality articles, social media posts, and marketing copy with ease. \n",
    "- **Research assistance**: Summarize complex documents, find relevant studies, and provide insightful analysis. \n",
    "- **Personal assistants**: Manage schedules, set reminders, and help you stay organized. \n",
    "- **Creative expression**: Write stories, poems, and even act as a brainstorming partner for new ideas. \n",
    "- **Data analysis**: Process and interpret large datasets to extract meaningful patterns and insights.  \n",
    "\n",
    "With the ability to configure your LLM-powered AI agent for different scenarios and outputs, you unlock an incredible tool that can adapt to various roles and industries. The potential applications are vast, limited only by your imagination and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "\n",
    "import tiktoken\n",
    "import requests\n",
    "from pydantic import BaseModel, Field, ValidationError, create_model\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from language_models.agent.output_parser import (\n",
    "    FINAL_ANSWER_INSTRUCTIONS,\n",
    "    AgentOutputParser,\n",
    "    LLMSingleCompletionFinalAnswer,\n",
    "    LLMChainOfThoughtFinalAnswer,\n",
    "    LLMToolUse,\n",
    "    OutputType,\n",
    "    get_schema_from_args,\n",
    ")\n",
    "from language_models.models.llm import ChatMessage, ChatMessageRole, OpenAILanguageModel\n",
    "from language_models.tools.tool import Tool\n",
    "from language_models.models.llm import ChatMessage\n",
    "from language_models.proxy_client import ProxyClient\n",
    "from language_models.settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%d/%m/%y %H:%M:%S\",\n",
    ")\n",
    "\n",
    "proxy_client = ProxyClient(\n",
    "    client_id=settings.CLIENT_ID,\n",
    "    client_secret=settings.CLIENT_SECRET,\n",
    "    auth_url=settings.AUTH_URL,\n",
    "    api_base=settings.API_BASE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILanguageModel(\n",
    "    proxy_client=proxy_client,\n",
    "    model='gpt-4',\n",
    "    max_tokens=1000,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code cell, we define instructions for an AI agent to follow when responding to different types of prompts: single completion, chain-of-thought with tools, and chain-of-thought without tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_COMPLETION_INSTRUCTIONS = \"\"\"### Instructions ###\n",
    "\n",
    "Your goal is to solve the problem you will be provided with\n",
    "\n",
    "You should respond with:\n",
    "```\n",
    "<response to the prompt>\n",
    "```\"\"\"\n",
    "\n",
    "CHAIN_OF_THOUGHT_INSTRUCTIONS_WITH_TOOLS = \"\"\"### Tools ###\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "### Instructions ###\n",
    "\n",
    "Your goal is to solve the problem you will be provided with\n",
    "\n",
    "You should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Tool: <name of the tool to use>\n",
    "\n",
    "Tool Input: <input of the tool to use>\n",
    "```\n",
    "\n",
    "Your <input of the tool to use> must be a JSON format with the keyword arguments of <name of the tool to use>\n",
    "\n",
    "When you know the final answer to the user's query you should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Final Answer: <response to the prompt>\n",
    "```\"\"\"\n",
    "\n",
    "CHAIN_OF_THOUGHT_INSTRUCTIONS_WITHOUT_TOOLS = \"\"\"### Instructions ###\n",
    "\n",
    "Your goal is to solve the problem you will be provided with\n",
    "\n",
    "You should respond with:\n",
    "```\n",
    "Thought: <thought process on how to respond to the prompt>\n",
    "\n",
    "Final Answer: <response to the prompt>\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a `Chat` class that manages the chat history and a list of intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat(BaseModel):\n",
    "    \"\"\"Class that implements the chat history.\"\"\"\n",
    "\n",
    "    messages: list[ChatMessage]\n",
    "    steps: list[str] = []\n",
    "\n",
    "    def update(self, prompt: str) -> None:\n",
    "        \"\"\"Modifies the user prompt to include intermediate steps.\"\"\"\n",
    "        self.messages[-1].content = prompt + \"\\n\\nThis was your previous work:\" + \"\\n\\n\".join(self.steps)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets the chat.\"\"\"\n",
    "        self.messages = [self.messages[0]]\n",
    "        self.steps = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of tokens in a conversation history to ensure it stays within the predefined limits for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TOKEN_LIMIT = {\n",
    "    \"gpt-4\": 8192,\n",
    "    \"gpt-4-32k\": 32768,\n",
    "    \"gpt-35-turbo\": 4096,\n",
    "    \"gpt-35-turbo-16k\": 16385,\n",
    "}\n",
    "\n",
    "def num_tokens_from_messages(messages: list[ChatMessage]) -> int:\n",
    "    \"\"\"Counts the number of tokens in the conversation history.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4\n",
    "        for key, value in message.model_dump().items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += -1\n",
    "    num_tokens += 2\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create an `Agent` class was implemented to encapsulate an LLM-powered AI agent, providing methods for managing conversation context, parsing outputs, and executing the agent logic. Finally, we introduced a `create` class method to instantiate an agent with various configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptingStrategy(str, Enum):\n",
    "    SINGLE_COMPLETION = \"single-completion\"\n",
    "    CHAIN_OF_THOUGHT = \"chain-of-thought\"\n",
    "\n",
    "class AgentOutput(BaseModel):\n",
    "    \"\"\"Class that represents the agent output.\"\"\"\n",
    "\n",
    "    prompt: str\n",
    "    final_answer: (\n",
    "        str\n",
    "        | int\n",
    "        | float\n",
    "        | dict[str, Any]\n",
    "        | BaseModel\n",
    "        | list[str]\n",
    "        | list[int]\n",
    "        | list[float]\n",
    "        | list[dict[str, Any]]\n",
    "        | list[BaseModel]\n",
    "        | None\n",
    "    )\n",
    "\n",
    "class Agent(BaseModel):\n",
    "    \"\"\"Class that implements an LLM-powered AI agent.\"\"\"\n",
    "\n",
    "    llm: OpenAILanguageModel\n",
    "    tools: dict[str, Tool] | None\n",
    "    prompt: str\n",
    "    prompt_variables: list[str]\n",
    "    output_parser: AgentOutputParser\n",
    "    chat: Chat\n",
    "    prompting_strategy: PromptingStrategy\n",
    "    iterations: int = 10\n",
    "    verbose: bool\n",
    "\n",
    "    def trim_conversation(self) -> None:\n",
    "        \"\"\"Trims the chat messages to fit the LLM context length.\"\"\"\n",
    "        num_tokens = num_tokens_from_messages(self.chat.messages)\n",
    "        while num_tokens + self.llm.max_tokens >= MODEL_TOKEN_LIMIT[self.llm.model]:\n",
    "            del self.chat.messages[1]\n",
    "            num_tokens = num_tokens_from_messages(self.chat.messages)\n",
    "\n",
    "    def parse_output(self, output: str) -> LLMToolUse | LLMSingleCompletionFinalAnswer | LLMChainOfThoughtFinalAnswer:\n",
    "        \"\"\"Parses the LLM output.\"\"\"\n",
    "        try:\n",
    "            output = self.output_parser.parse(output)\n",
    "            observation = None\n",
    "        except (ValueError, ValidationError) as error:\n",
    "            output = None\n",
    "            observation = error\n",
    "        return output, observation\n",
    "\n",
    "    def invoke(self, prompt: dict[str, Any]) -> AgentOutput:\n",
    "        \"\"\"Runs the agent given a prompt.\"\"\"\n",
    "        prompt = self.prompt.format(**{variable: prompt.get(variable) for variable in self.prompt_variables})\n",
    "        if self.verbose:\n",
    "            logging.info(\"Prompt:\\n%s\", prompt)\n",
    "\n",
    "        self.chat.messages.append(ChatMessage(role=ChatMessageRole.USER, content=prompt))\n",
    "        self.chat.steps = []\n",
    "\n",
    "        iteration = 0\n",
    "        while iteration <= self.iterations:\n",
    "            self.trim_conversation()\n",
    "            output = self.llm.get_completion(self.chat.messages)\n",
    "            if self.verbose:\n",
    "                logging.info(\"Raw Output:\\n%s\", output)\n",
    "\n",
    "            output, observation = self.parse_output(output)\n",
    "\n",
    "            if self.prompting_strategy == PromptingStrategy.CHAIN_OF_THOUGHT:\n",
    "                if output is not None:\n",
    "                    if self.verbose:\n",
    "                        logging.info(\"Thought:\\n%s\", output.thought)\n",
    "\n",
    "                    self.chat.steps.append(f\"Thought: {output.thought}\")\n",
    "\n",
    "                    if isinstance(output, LLMChainOfThoughtFinalAnswer):\n",
    "                        if self.verbose:\n",
    "                            logging.info(\"Final Answer:\\n%s\", output.final_answer)\n",
    "\n",
    "                        self.chat.messages.append(\n",
    "                            ChatMessage(role=ChatMessageRole.ASSISTANT, content=str(output.final_answer))\n",
    "                        )\n",
    "                        return AgentOutput(prompt=prompt, final_answer=output.final_answer)\n",
    "\n",
    "                    else:\n",
    "                        if self.tools is not None:\n",
    "                            if self.verbose:\n",
    "                                logging.info(\"Tool:\\n%s\", output.tool)\n",
    "                                logging.info(\"Tool Input:\\n%s\", output.tool_input)\n",
    "\n",
    "                            tool = self.tools.get(output.tool)\n",
    "                            if tool is not None:\n",
    "                                tool_output = tool.invoke(output.tool_input)\n",
    "                                observation = f\"Tool Output:\\n{tool_output}\"\n",
    "                                if self.verbose:\n",
    "                                    logging.info(observation)\n",
    "\n",
    "                                self.chat.steps.append(f\"Tool: {tool.name}\")\n",
    "                                self.chat.steps.append(f\"Tool Input: {output.tool_input}\")\n",
    "\n",
    "                            else:\n",
    "                                tool_names = \", \".join(list(self.tools.keys()))\n",
    "                                observation = f\"{output.tool} tool doesn't exist. Try one of these tools: {tool_names}\"\n",
    "\n",
    "                self.chat.steps.append(f\"Observation: {observation}\")\n",
    "                self.chat.update(prompt)\n",
    "\n",
    "            else:\n",
    "                if isinstance(output, LLMSingleCompletionFinalAnswer):\n",
    "                    if self.verbose:\n",
    "                        logging.info(\"Final Answer:\\n%s\", output.final_answer)\n",
    "\n",
    "                    self.chat.messages.append(\n",
    "                        ChatMessage(role=ChatMessageRole.ASSISTANT, content=str(output.final_answer))\n",
    "                    )\n",
    "                    return AgentOutput(prompt=prompt, final_answer=output.final_answer)\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        if self.output_parser.output_type == OutputType.STRUCT:\n",
    "            final_answer = {key: None for key in self.output_parser.output_schema.model_json_schema()[\"properties\"]}\n",
    "        elif self.output_parser.output_type == OutputType.ARRAY_STRUCT:\n",
    "            final_answer = [{key: None for key in self.output_parser.output_schema.model_json_schema()[\"properties\"]}]\n",
    "        elif self.output_parser.output_type in (OutputType.OBJECT, OutputType.ARRAY_OBJECT):\n",
    "            fields = self.output_parser.output_schema.__annotations__\n",
    "            optional_fields = {field: (data_type | None, None) for field, data_type in fields.items()}\n",
    "            model = create_model(self.output_parser.output_schema.__name__, **optional_fields)\n",
    "            final_answer = model() if self.output_parser.output_type == OutputType.OBJECT else [model()]\n",
    "        else:\n",
    "            final_answer = None\n",
    "\n",
    "        if self.verbose:\n",
    "            logging.info(\"Final Answer:\\n%s\", final_answer)\n",
    "\n",
    "        return AgentOutput(prompt=prompt, final_answer=final_answer)\n",
    "\n",
    "    @classmethod\n",
    "    def create(\n",
    "        cls,\n",
    "        llm: OpenAILanguageModel,\n",
    "        system_prompt: str,\n",
    "        prompt: str,\n",
    "        prompt_variables: list[str],\n",
    "        output_type: OutputType,\n",
    "        output_schema: type[BaseModel] | str | None = None,\n",
    "        tools: list[Tool] | None = None,\n",
    "        prompting_strategy: PromptingStrategy = PromptingStrategy.CHAIN_OF_THOUGHT,\n",
    "        verbose: bool = True,\n",
    "    ) -> Agent:\n",
    "        \"\"\"Creates an instance of the ReAct agent.\"\"\"\n",
    "        if prompting_strategy == PromptingStrategy.CHAIN_OF_THOUGHT:\n",
    "            if tools is None:\n",
    "                instructions = CHAIN_OF_THOUGHT_INSTRUCTIONS_WITHOUT_TOOLS\n",
    "                tool_use = False\n",
    "                tools = None\n",
    "                iterations = 5\n",
    "            else:\n",
    "                instructions = CHAIN_OF_THOUGHT_INSTRUCTIONS_WITH_TOOLS.format(\n",
    "                    tools=\"\\n\\n\".join([str(tool) for tool in tools])\n",
    "                )\n",
    "                tool_use = True\n",
    "                tools = {tool.name: tool for tool in tools}\n",
    "                iterations = max(5, len(tools) * 2)\n",
    "        else:\n",
    "            instructions = SINGLE_COMPLETION_INSTRUCTIONS\n",
    "            tool_use = False\n",
    "            iterations = 1\n",
    "\n",
    "        if output_type in (OutputType.OBJECT, OutputType.ARRAY_OBJECT):\n",
    "            if output_schema is None:\n",
    "                raise ValueError(f\"When using {output_type} as the output type a schema must be provided.\")\n",
    "\n",
    "            args = output_schema.model_json_schema()[\"properties\"]\n",
    "            final_answer_instructions = FINAL_ANSWER_INSTRUCTIONS[output_type].format(\n",
    "                output_schema=get_schema_from_args(args)\n",
    "            )\n",
    "        elif output_type in (OutputType.DATE, OutputType.TIMESTAMP):\n",
    "            final_answer_instructions = FINAL_ANSWER_INSTRUCTIONS[output_type].format(output_schema=output_schema)\n",
    "        else:\n",
    "            final_answer_instructions = FINAL_ANSWER_INSTRUCTIONS[output_type]\n",
    "\n",
    "        chat = Chat(\n",
    "            messages=[\n",
    "                ChatMessage(\n",
    "                    role=ChatMessageRole.SYSTEM,\n",
    "                    content=\"\\n\\n\".join([system_prompt, instructions, final_answer_instructions]),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        output_parser = AgentOutputParser(\n",
    "            output_type=output_type,\n",
    "            output_schema=output_schema,\n",
    "            prompting_strategy=prompting_strategy,\n",
    "            tool_use=tool_use,\n",
    "        )\n",
    "\n",
    "        return Agent(\n",
    "            llm=llm,\n",
    "            tools=tools,\n",
    "            prompt=prompt,\n",
    "            prompt_variables=prompt_variables,\n",
    "            output_parser=output_parser,\n",
    "            chat=chat,\n",
    "            prompting_strategy=prompting_strategy,\n",
    "            iterations=iterations,\n",
    "            verbose=verbose,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Completion\n",
    "\n",
    "The following code creates an AI agent that can answer user questions based on the internal knowledge of the LLM. It's designed to provide precise and detailed responses by leveraging a single completion strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an AI assistant designed to help users with a variety of tasks.\"\n",
    "\n",
    "agent = Agent.create(\n",
    "    llm=llm,\n",
    "    system_prompt=system_prompt,\n",
    "    prompt=\"{question}\",\n",
    "    prompt_variables=[\"question\"],\n",
    "    output_type=OutputType.STRING,\n",
    "    prompting_strategy=PromptingStrategy.SINGLE_COMPLETION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/08/24 14:24:47 INFO Prompt:\n",
      "Hi, can you help me understand the basics of quantum computing?\n",
      "06/08/24 14:24:59 INFO Raw Output:\n",
      "Sure, I'd be happy to help explain the basics of quantum computing!\n",
      "\n",
      "Quantum computing is a type of computation that uses quantum bits, or 'qubits', instead of the classical bits used in traditional computing. While classical bits can be either a 0 or a 1, a qubit can be both at the same time, thanks to a property called superposition.\n",
      "\n",
      "Another key principle of quantum computing is entanglement. When qubits become entangled, the state of one qubit becomes connected to the state of another. Changing the state of one qubit will instantaneously change the state of the other, no matter how far apart they are.\n",
      "\n",
      "These properties allow quantum computers to process a high number of possibilities at once, solve certain problems much faster than classical computers, and perform complex calculations that are currently impossible with classical computers.\n",
      "\n",
      "However, quantum computing is still in its early stages and there are many technical challenges to overcome before it can be used for practical purposes. Quantum computers are highly sensitive to changes in their environment, which can cause errors in calculations. This is known as 'quantum decoherence'.\n",
      "\n",
      "In summary, quantum computing is a promising field that could revolutionize many areas, from cryptography to drug discovery, but it's still very much in the research and development stage.\n",
      "06/08/24 14:24:59 INFO Final Answer:\n",
      "Sure, I'd be happy to help explain the basics of quantum computing!\n",
      "\n",
      "Quantum computing is a type of computation that uses quantum bits, or 'qubits', instead of the classical bits used in traditional computing. While classical bits can be either a 0 or a 1, a qubit can be both at the same time, thanks to a property called superposition.\n",
      "\n",
      "Another key principle of quantum computing is entanglement. When qubits become entangled, the state of one qubit becomes connected to the state of another. Changing the state of one qubit will instantaneously change the state of the other, no matter how far apart they are.\n",
      "\n",
      "These properties allow quantum computers to process a high number of possibilities at once, solve certain problems much faster than classical computers, and perform complex calculations that are currently impossible with classical computers.\n",
      "\n",
      "However, quantum computing is still in its early stages and there are many technical challenges to overcome before it can be used for practical purposes. Quantum computers are highly sensitive to changes in their environment, which can cause errors in calculations. This is known as 'quantum decoherence'.\n",
      "\n",
      "In summary, quantum computing is a promising field that could revolutionize many areas, from cryptography to drug discovery, but it's still very much in the research and development stage.\n"
     ]
    }
   ],
   "source": [
    "output = agent.invoke({\"question\": \"Hi, can you help me understand the basics of quantum computing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd be happy to help explain the basics of quantum computing!\n",
      "\n",
      "Quantum computing is a type of computation that uses quantum bits, or 'qubits', instead of the classical bits used in traditional computing. While classical bits can be either a 0 or a 1, a qubit can be both at the same time, thanks to a property called superposition.\n",
      "\n",
      "Another key principle of quantum computing is entanglement. When qubits become entangled, the state of one qubit becomes connected to the state of another. Changing the state of one qubit will instantaneously change the state of the other, no matter how far apart they are.\n",
      "\n",
      "These properties allow quantum computers to process a high number of possibilities at once, solve certain problems much faster than classical computers, and perform complex calculations that are currently impossible with classical computers.\n",
      "\n",
      "However, quantum computing is still in its early stages and there are many technical challenges to overcome before it can be used for practical purposes. Quantum computers are highly sensitive to changes in their environment, which can cause errors in calculations. This is known as 'quantum decoherence'.\n",
      "\n",
      "In summary, quantum computing is a promising field that could revolutionize many areas, from cryptography to drug discovery, but it's still very much in the research and development stage.\n"
     ]
    }
   ],
   "source": [
    "print(output.final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/08/24 14:24:59 INFO Prompt:\n",
      "Alex has three times as many apples as Ben. Together, they have 24 apples. How many apples does each person have?\n",
      "06/08/24 14:25:06 INFO Raw Output:\n",
      "Let's denote the number of apples Ben has as x. According to the problem, Alex has 3 times as many apples as Ben, so Alex has 3x apples. \n",
      "\n",
      "Together, they have 24 apples. So we can set up the following equation: \n",
      "\n",
      "x (Ben's apples) + 3x (Alex's apples) = 24\n",
      "\n",
      "Solving this equation gives:\n",
      "\n",
      "4x = 24\n",
      "x = 24 / 4\n",
      "x = 6\n",
      "\n",
      "So, Ben has 6 apples and Alex, having three times as many, has 18 apples.\n",
      "06/08/24 14:25:06 INFO Final Answer:\n",
      "Let's denote the number of apples Ben has as x. According to the problem, Alex has 3 times as many apples as Ben, so Alex has 3x apples. \n",
      "\n",
      "Together, they have 24 apples. So we can set up the following equation: \n",
      "\n",
      "x (Ben's apples) + 3x (Alex's apples) = 24\n",
      "\n",
      "Solving this equation gives:\n",
      "\n",
      "4x = 24\n",
      "x = 24 / 4\n",
      "x = 6\n",
      "\n",
      "So, Ben has 6 apples and Alex, having three times as many, has 18 apples.\n"
     ]
    }
   ],
   "source": [
    "output = agent.invoke({\"question\": \"Alex has three times as many apples as Ben. Together, they have 24 apples. How many apples does each person have?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's denote the number of apples Ben has as x. According to the problem, Alex has 3 times as many apples as Ben, so Alex has 3x apples. \n",
      "\n",
      "Together, they have 24 apples. So we can set up the following equation: \n",
      "\n",
      "x (Ben's apples) + 3x (Alex's apples) = 24\n",
      "\n",
      "Solving this equation gives:\n",
      "\n",
      "4x = 24\n",
      "x = 24 / 4\n",
      "x = 6\n",
      "\n",
      "So, Ben has 6 apples and Alex, having three times as many, has 18 apples.\n"
     ]
    }
   ],
   "source": [
    "print(output.final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought\n",
    "\n",
    "The following code configures an LLM to specialize in answering earthquake-related questions by simulating the expertise of a United States Geological Survey (USGS) expert. It integrates various tools, including those for earthquake information and current date retrieval, to enhance its responses. Additionally, the LLM's output is formatted according to a specified schema, ensuring clarity and consistency in its responses. The structured output format of the final answer will become more useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_date() -> datetime:\n",
    "    return datetime.now()\n",
    "\n",
    "current_date_tool = Tool(\n",
    "    function=current_date,\n",
    "    name=\"Current Date\",\n",
    "    description=\"Use this tool to access the current local date and time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USGeopoliticalSurveyEarthquakeAPI(BaseModel):\n",
    "    \"\"\"Class that implements the API interface.\"\"\"\n",
    "\n",
    "    start_time: str = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Limit to events on or after the specified start time. NOTE: All times use ISO8601 Date/Time format.\"\n",
    "            + \" Unless a timezone is specified, UTC is assumed.\"\n",
    "        ),\n",
    "    )\n",
    "    end_time: str = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Limit to events on or before the specified end time. NOTE: All times use ISO8601 Date/Time format.\"\n",
    "            + \" Unless a timezone is specified, UTC is assumed.\"\n",
    "        ),\n",
    "    )\n",
    "    limit: int = Field(\n",
    "        20000,\n",
    "        description=(\n",
    "            \"Limit the results to the specified number of events. NOTE: The service limits queries to 20000,\"\n",
    "            + \" and any that exceed this limit will generate a HTTP response code 400 Bad Request.\"\n",
    "        ),\n",
    "    )\n",
    "    min_depth: int = Field(\n",
    "        -100,\n",
    "        description=\"Limit to events with depth more than the specified minimum.\",\n",
    "    )\n",
    "    max_depth: int = Field(\n",
    "        1000,\n",
    "        description=\"Limit to events with depth less than the specified maximum.\",\n",
    "    )\n",
    "    min_magnitude: int = Field(\n",
    "        None,\n",
    "        description=\"Limit to events with a magnitude larger than the specified minimum.\",\n",
    "    )\n",
    "    max_magnitude: int = Field(\n",
    "        None,\n",
    "        description=\"Limit to events with a magnitude smaller than the specified maximum.\",\n",
    "    )\n",
    "    alert_level: str = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Limit to events with a specific PAGER alert level.\"\n",
    "            + \" The allowed values are: alert_level=green Limit to events with PAGER\"\n",
    "            + ' alert level \"green\". alert_level=yellow Limit to events with PAGER alert level \"yellow\".'\n",
    "            + ' alert_level=orange Limit to events with PAGER alert level \"orange\".'\n",
    "            + ' alert_level=red Limit to events with PAGER alert level \"red\".'\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def get_earthquakes(\n",
    "    endpoint: str,\n",
    "    start_time: datetime = (datetime.now() - timedelta(days=30)).date(),\n",
    "    end_time: datetime = datetime.now().date(),\n",
    "    limit: int = 20000,\n",
    "    min_depth: int = -100,\n",
    "    max_depth: int = 1000,\n",
    "    min_magnitude: int | None = None,\n",
    "    max_magnitude: int | None = None,\n",
    "    alert_level: str | None = None,\n",
    ") -> Any:\n",
    "    params = {\n",
    "        \"format\": \"geojson\",\n",
    "        \"starttime\": start_time,\n",
    "        \"endtime\": end_time,\n",
    "        \"limit\": limit,\n",
    "        \"mindepth\": min_depth,\n",
    "        \"maxdepth\": max_depth,\n",
    "        \"minmagnitude\": min_magnitude,\n",
    "        \"maxmagnitude\": max_magnitude,\n",
    "        \"alertlevel\": alert_level,\n",
    "        \"eventtype\": \"earthquake\",\n",
    "    }\n",
    "    response = requests.get(\n",
    "        f\"https://earthquake.usgs.gov/fdsnws/event/1/{endpoint}\",\n",
    "        params=params,\n",
    "        timeout=None,\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "def query_earthquakes(\n",
    "    start_time: datetime = (datetime.now() - timedelta(days=30)).date(),\n",
    "    end_time: datetime = datetime.now().date(),\n",
    "    limit: int = 20000,\n",
    "    min_depth: int = -100,\n",
    "    max_depth: int = 1000,\n",
    "    min_magnitude: int | None = None,\n",
    "    max_magnitude: int | None = None,\n",
    "    alert_level: str | None = None,\n",
    ") -> Any:\n",
    "    return get_earthquakes(\n",
    "        endpoint=\"query\",\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        limit=limit,\n",
    "        min_depth=min_depth,\n",
    "        max_depth=max_depth,\n",
    "        min_magnitude=min_magnitude,\n",
    "        max_magnitude=max_magnitude,\n",
    "        alert_level=alert_level,\n",
    "    )\n",
    "\n",
    "def count_earthquakes(\n",
    "    start_time: datetime = (datetime.now() - timedelta(days=30)).date(),\n",
    "    end_time: datetime = datetime.now().date(),\n",
    "    limit: int = 20000,\n",
    "    min_depth: int = -100,\n",
    "    max_depth: int = 1000,\n",
    "    min_magnitude: int | None = None,\n",
    "    max_magnitude: int | None = None,\n",
    "    alert_level: str | None = None,\n",
    ") -> Any:\n",
    "    return get_earthquakes(\n",
    "        endpoint=\"count\",\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        limit=limit,\n",
    "        min_depth=min_depth,\n",
    "        max_depth=max_depth,\n",
    "        min_magnitude=min_magnitude,\n",
    "        max_magnitude=max_magnitude,\n",
    "        alert_level=alert_level,\n",
    "    )\n",
    "\n",
    "query_earthquakes_tool = Tool(\n",
    "    function=query_earthquakes,\n",
    "    name=\"Query Earthquakes\",\n",
    "    description=\"Use this tool to search recent earthquakes\",\n",
    "    args_schema=USGeopoliticalSurveyEarthquakeAPI,\n",
    ")\n",
    "\n",
    "count_earthquakes_tool = Tool(\n",
    "    function=count_earthquakes,\n",
    "    name=\"Count Earthquakes\",\n",
    "    description=\"Use this tool to count and aggregate recent earthquakes\",\n",
    "    args_schema=USGeopoliticalSurveyEarthquakeAPI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an United States Geological Survey expert who can answer questions regarding earthquakes.\"\n",
    "\n",
    "agent = Agent.create(\n",
    "    llm=llm,\n",
    "    system_prompt=system_prompt,\n",
    "    prompt=\"{question}\",\n",
    "    prompt_variables=[\"question\"],\n",
    "    output_type=OutputType.STRING,\n",
    "    tools=[current_date_tool, count_earthquakes_tool, query_earthquakes_tool],\n",
    "    prompting_strategy=PromptingStrategy.CHAIN_OF_THOUGHT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following question demonstrates how the LLM processes a user's query about the number of earthquakes that occurred on the current day. Using the provided tools, the LLM first retrieves the current date and time. This initial step is crucial for identifying and counting the seismic activities for the day. This process shows the power of Chain-of-Thought, particularly the ReAct prompting method, which enables the AI to tackle multi-step problems. By breaking down the task into smaller, manageable steps - first obtaining the current date, then querying the relevant seismic data - the LLM can deliver correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/08/24 14:25:06 INFO Prompt:\n",
      "How many earthquakes occurred today?\n",
      "06/08/24 14:25:09 INFO Raw Output:\n",
      "Thought: To answer this question, I need to use the \"Count Earthquakes\" tool. However, I first need to know today's date, which I can get using the \"Current Date\" tool.\n",
      "\n",
      "Tool: Current Date\n",
      "\n",
      "Tool Input: {}\n",
      "06/08/24 14:25:09 INFO Thought:\n",
      "To answer this question, I need to use the \"Count Earthquakes\" tool. However, I first need to know today's date, which I can get using the \"Current Date\" tool.\n",
      "06/08/24 14:25:09 INFO Tool:\n",
      "Current Date\n",
      "06/08/24 14:25:09 INFO Tool Input:\n",
      "{}\n",
      "06/08/24 14:25:09 INFO Tool Output:\n",
      "2024-08-06 14:25:09.676058\n",
      "06/08/24 14:25:14 INFO Raw Output:\n",
      "Thought: Now that I have today's date, I can use the \"Count Earthquakes\" tool to find out how many earthquakes occurred today.\n",
      "\n",
      "Tool: Count Earthquakes\n",
      "\n",
      "Tool Input: {\"start_date\": \"2024-08-06\", \"end_date\": \"2024-08-06\"}\n",
      "06/08/24 14:25:14 INFO Thought:\n",
      "Now that I have today's date, I can use the \"Count Earthquakes\" tool to find out how many earthquakes occurred today.\n",
      "06/08/24 14:25:14 INFO Tool:\n",
      "Count Earthquakes\n",
      "06/08/24 14:25:14 INFO Tool Input:\n",
      "{'start_date': '2024-08-06', 'end_date': '2024-08-06'}\n",
      "06/08/24 14:25:15 INFO Tool Output:\n",
      "{'count': 9849, 'maxAllowed': 20000}\n",
      "06/08/24 14:25:17 INFO Raw Output:\n",
      "Thought: The tool has returned the number of earthquakes that occurred today. \n",
      "\n",
      "Final Answer: There were 9849 earthquakes today.\n",
      "06/08/24 14:25:17 INFO Thought:\n",
      "The tool has returned the number of earthquakes that occurred today.\n",
      "06/08/24 14:25:17 INFO Final Answer:\n",
      "There were 9849 earthquakes today.\n"
     ]
    }
   ],
   "source": [
    "output = agent.invoke({\"question\": \"How many earthquakes occurred today?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 9849 earthquakes today.\n"
     ]
    }
   ],
   "source": [
    "print(output.final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we give the LLM a follow-up question where `Show me 3` refers to the earthquakes that occurred today. By keeping track of the chat history, the LLM can understand the context and continuity of the conversation. This enables the LLM to recognize that \"today\" is the time frame in question and to provide details about three specific earthquakes that occurred on the current day. This capability to maintain and utilize chat history is essential for multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/08/24 14:25:17 INFO Prompt:\n",
      "Show me 3.\n",
      "06/08/24 14:25:25 INFO Raw Output:\n",
      "Thought: To show details of 3 earthquakes, I can use the \"Query Earthquakes\" tool. I already have the date from the previous query, so I can use that as the input.\n",
      "\n",
      "Tool: Query Earthquakes\n",
      "\n",
      "Tool Input: {\"start_date\": \"2024-08-06\", \"end_date\": \"2024-08-06\", \"limit\": 3}\n",
      "06/08/24 14:25:25 INFO Thought:\n",
      "To show details of 3 earthquakes, I can use the \"Query Earthquakes\" tool. I already have the date from the previous query, so I can use that as the input.\n",
      "06/08/24 14:25:25 INFO Tool:\n",
      "Query Earthquakes\n",
      "06/08/24 14:25:25 INFO Tool Input:\n",
      "{'start_date': '2024-08-06', 'end_date': '2024-08-06', 'limit': 3}\n",
      "06/08/24 14:25:25 INFO Tool Output:\n",
      "{'type': 'FeatureCollection', 'metadata': {'generated': 1722947125000, 'url': 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2024-07-07&endtime=2024-08-06&limit=3&mindepth=-100&maxdepth=1000&eventtype=earthquake', 'title': 'USGS Earthquakes', 'status': 200, 'api': '1.14.1', 'limit': 3, 'offset': 1, 'count': 3}, 'features': [{'type': 'Feature', 'properties': {'mag': 2.08, 'place': '9 km S of Fern Forest, Hawaii', 'time': 1722902326080, 'updated': 1722902483720, 'tz': None, 'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/hv74386761', 'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=hv74386761&format=geojson', 'felt': None, 'cdi': None, 'mmi': None, 'alert': None, 'status': 'automatic', 'tsunami': 0, 'sig': 67, 'net': 'hv', 'code': '74386761', 'ids': ',hv74386761,', 'sources': ',hv,', 'types': ',origin,phase-data,', 'nst': 33, 'dmin': 0.01996, 'rms': 0.25, 'gap': 85, 'magType': 'ml', 'type': 'earthquake', 'title': 'M 2.1 - 9 km S of Fern Forest, Hawaii'}, 'geometry': {'type': 'Point', 'coordinates': [-155.148162841797, 19.3801670074463, 2.73000001907349]}, 'id': 'hv74386761'}, {'type': 'Feature', 'properties': {'mag': 1.11, 'place': '11 km N of Borrego Springs, CA', 'time': 1722901879940, 'updated': 1722902250414, 'tz': None, 'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/ci40863608', 'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=ci40863608&format=geojson', 'felt': None, 'cdi': None, 'mmi': None, 'alert': None, 'status': 'automatic', 'tsunami': 0, 'sig': 19, 'net': 'ci', 'code': '40863608', 'ids': ',ci40863608,', 'sources': ',ci,', 'types': ',nearby-cities,origin,phase-data,scitech-link,', 'nst': 63, 'dmin': 0.02606, 'rms': 0.2, 'gap': 30, 'magType': 'ml', 'type': 'earthquake', 'title': 'M 1.1 - 11 km N of Borrego Springs, CA'}, 'geometry': {'type': 'Point', 'coordinates': [-116.3656667, 33.3528333, 11.24]}, 'id': 'ci40863608'}, {'type': 'Feature', 'properties': {'mag': 4, 'place': '19 km S of Camiña, Chile', 'time': 1722900668377, 'updated': 1722907771098, 'tz': None, 'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/us6000ni9y', 'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=us6000ni9y&format=geojson', 'felt': 1, 'cdi': 1, 'mmi': None, 'alert': None, 'status': 'reviewed', 'tsunami': 0, 'sig': 246, 'net': 'us', 'code': '6000ni9y', 'ids': ',us6000ni9y,', 'sources': ',us,', 'types': ',dyfi,origin,phase-data,', 'nst': 16, 'dmin': 0.306, 'rms': 0.35, 'gap': 103, 'magType': 'mb', 'type': 'earthquake', 'title': 'M 4.0 - 19 km S of Camiña, Chile'}, 'geometry': {'type': 'Point', 'coordinates': [-69.4577, -19.4891, 96.285]}, 'id': 'us6000ni9y'}], 'bbox': [-155.1481628418, -19.4891, 2.7300000190735, -69.4577, 33.3528333, 96.285]}\n",
      "06/08/24 14:25:33 INFO Raw Output:\n",
      "Here are the details of 3 earthquakes that occurred today:\n",
      "\n",
      "1. A magnitude 2.08 earthquake occurred 9 km south of Fern Forest, Hawaii. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/hv74386761).\n",
      "\n",
      "2. A magnitude 1.11 earthquake occurred 11 km north of Borrego Springs, CA. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/ci40863608).\n",
      "\n",
      "3. A magnitude 4.0 earthquake occurred 19 km south of Camiña, Chile. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/us6000ni9y).\n",
      "06/08/24 14:25:43 INFO Raw Output:\n",
      "Thought: The tool output contains the details of three earthquakes. I can extract the necessary information and present it to the user.\n",
      "\n",
      "Final Answer: Here are the details of 3 earthquakes that occurred today:\n",
      "\n",
      "1. A magnitude 2.08 earthquake occurred 9 km south of Fern Forest, Hawaii. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/hv74386761).\n",
      "\n",
      "2. A magnitude 1.11 earthquake occurred 11 km north of Borrego Springs, CA. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/ci40863608).\n",
      "\n",
      "3. A magnitude 4.0 earthquake occurred 19 km south of Camiña, Chile. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/us6000ni9y).\n",
      "06/08/24 14:25:43 INFO Thought:\n",
      "The tool output contains the details of three earthquakes. I can extract the necessary information and present it to the user.\n",
      "06/08/24 14:25:43 INFO Final Answer:\n",
      "Here are the details of 3 earthquakes that occurred today:\n",
      "\n",
      "1. A magnitude 2.08 earthquake occurred 9 km south of Fern Forest, Hawaii. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/hv74386761).\n",
      "\n",
      "2. A magnitude 1.11 earthquake occurred 11 km north of Borrego Springs, CA. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/ci40863608).\n",
      "\n",
      "3. A magnitude 4.0 earthquake occurred 19 km south of Camiña, Chile. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/us6000ni9y).\n"
     ]
    }
   ],
   "source": [
    "output = agent.invoke({\"question\": \"Show me 3.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the details of 3 earthquakes that occurred today:\n",
      "\n",
      "1. A magnitude 2.08 earthquake occurred 9 km south of Fern Forest, Hawaii. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/hv74386761).\n",
      "\n",
      "2. A magnitude 1.11 earthquake occurred 11 km north of Borrego Springs, CA. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/ci40863608).\n",
      "\n",
      "3. A magnitude 4.0 earthquake occurred 19 km south of Camiña, Chile. More details can be found [here](https://earthquake.usgs.gov/earthquakes/eventpage/us6000ni9y).\n"
     ]
    }
   ],
   "source": [
    "print(output.final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM can also handle straightforward informational questions that don't require the use of additional tools. In this case, the question about the possibility of MegaQuakes (magnitude 10 or larger) can be answered directly by the language model without invoking any external tools. This shows the LLM's ability to recognize when it can rely on its internal knowledge base to provide an accurate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/08/24 14:25:43 INFO Prompt:\n",
      "Can MegaQuakes really happen? Like a magnitude 10 or larger?\n",
      "06/08/24 14:25:52 INFO Raw Output:\n",
      "Thought: The user is asking about the possibility of a magnitude 10 or larger earthquake, also known as a \"MegaQuake\". This is a general knowledge question about earthquakes and doesn't require the use of a tool.\n",
      "\n",
      "Final Answer: Theoretically, a magnitude 10 earthquake is possible, but it's extremely unlikely. The largest earthquake ever recorded was a magnitude 9.5 in Chile in 1960. A magnitude 10 earthquake would require a fault line capable of rupturing along a length of about 1000 km, and there are very few places on Earth where this could happen. Additionally, the energy required for such an event is about 30 times greater than the energy of a magnitude 9 earthquake. So while it's not impossible, it's highly unlikely that a magnitude 10 or larger earthquake will occur.\n",
      "06/08/24 14:25:52 INFO Thought:\n",
      "The user is asking about the possibility of a magnitude 10 or larger earthquake, also known as a \"MegaQuake\". This is a general knowledge question about earthquakes and doesn't require the use of a tool.\n",
      "06/08/24 14:25:52 INFO Final Answer:\n",
      "Theoretically, a magnitude 10 earthquake is possible, but it's extremely unlikely. The largest earthquake ever recorded was a magnitude 9.5 in Chile in 1960. A magnitude 10 earthquake would require a fault line capable of rupturing along a length of about 1000 km, and there are very few places on Earth where this could happen. Additionally, the energy required for such an event is about 30 times greater than the energy of a magnitude 9 earthquake. So while it's not impossible, it's highly unlikely that a magnitude 10 or larger earthquake will occur.\n"
     ]
    }
   ],
   "source": [
    "output = agent.invoke({\"question\": \"Can MegaQuakes really happen? Like a magnitude 10 or larger?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretically, a magnitude 10 earthquake is possible, but it's extremely unlikely. The largest earthquake ever recorded was a magnitude 9.5 in Chile in 1960. A magnitude 10 earthquake would require a fault line capable of rupturing along a length of about 1000 km, and there are very few places on Earth where this could happen. Additionally, the energy required for such an event is about 30 times greater than the energy of a magnitude 9 earthquake. So while it's not impossible, it's highly unlikely that a magnitude 10 or larger earthquake will occur.\n"
     ]
    }
   ],
   "source": [
    "print(output.final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-powered-ai-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
