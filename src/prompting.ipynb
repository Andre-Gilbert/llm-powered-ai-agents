{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "To get the best results from LLMs, it's important to craft clear and effective prompts. Prompt engineering is an iterative process. Start with something simple and add more details later. Things to consider:\n",
    "\n",
    "1. **Be specific:** Provide detailed information to help the LLM understand your query and give tailored responses.\n",
    "2. **Ask clear questions:** Ask one question at a time to minimize confusion.\n",
    "3. **Ask follow-up questions:** Clarify incomplete or unclear initial responses with rephrased queries or additional context.\n",
    "4. **Use full sentences:** Provide context with clear and concise sentences.\n",
    "5. **Provide examples:** Use examples to help the LLM understand your requirements and respond appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_models.models.llm import OpenAILanguageModel, ChatMessage, ChatMessageRole\n",
    "from language_models.proxy_client import ProxyClient\n",
    "from language_models.settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_client = ProxyClient(\n",
    "    client_id=settings.CLIENT_ID,\n",
    "    client_secret=settings.CLIENT_SECRET,\n",
    "    auth_url=settings.AUTH_URL,\n",
    "    api_base=settings.API_BASE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILanguageModel(\n",
    "    proxy_client=proxy_client,\n",
    "    model=\"gpt-4\",\n",
    "    max_tokens=256,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt (Instructions)\n",
    "\n",
    "Set general behavior guidelines, such as the persona the LLM should adopt or how it should handle specific tasks and edge cases. For instance, instruct the LLM to check the date before other tasks or respond in a specific format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Prompt (Input Data)\n",
    "\n",
    "For simple queries, a direct question might suffice. For complex tasks, use structured prompts. For example, when classifying an IT ticket, provide clear details about the ticket, user, and date to ensure accurate handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
